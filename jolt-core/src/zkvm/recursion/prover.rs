//! Unified prover for the recursion SNARK protocol
//!
//! This module provides a high-level prover that orchestrates:
//! - Stage 1: Packed GT exp sumcheck
//! - Stage 2: Batched constraint sumchecks (shift + claim reduction + remaining constraints)
//! - Stage 3: Prefix packing reduction to a single dense polynomial opening
//!
//! The prover returns a proof and opening accumulator for PCS verification.

use crate::{
    field::JoltField,
    poly::{
        commitment::{
            commitment_scheme::{CommitmentScheme, RecursionExt},
            dory::{wrappers::ArkDoryProof, ArkworksVerifierSetup, DoryCommitmentScheme},
            hyrax::{matrix_dimensions, Hyrax, HyraxCommitment, PedersenGenerators},
        },
        dense_mlpoly::DensePolynomial,
        multilinear_polynomial::MultilinearPolynomial,
        opening_proof::{Openings, ProverOpeningAccumulator, SumcheckId},
    },
    transcripts::Transcript,
    zkvm::{
        proof_serialization::{PairingBoundary, RecursionConstraintMetadata},
        witness::{CommittedPolynomial, VirtualPolynomial},
    },
};
use crate::zkvm::proof_serialization::NonInputBaseHints;
use ark_bn254::{Fq, Fr};
use ark_grumpkin::Projective as GrumpkinProjective;
use ark_ff::Zero;
use ark_serialize::{CanonicalDeserialize, CanonicalSerialize};
use dory::backends::arkworks::ArkGT;
use rayon::prelude::*;
use std::collections::HashMap;
use std::sync::{Arc, Mutex};

use crate::zkvm::recursion::DoryMatrixBuilder;
use dory::backends::arkworks::BN254;
use dory::recursion::{ast::AstGraph, WitnessCollection};
use jolt_optimizations::{fq12_to_multilinear_evals, get_g_mle};

use super::{
    constraints::system::{ConstraintSystem, PolyType},
    g1::{
        addition::{G1AddParams, G1AddProver, G1AddProverSpec},
        scalar_multiplication::{
            G1ScalarMulConstraintPolynomials, G1ScalarMulParams, G1ScalarMulProver,
            G1ScalarMulProverSpec, G1ScalarMulPublicInputs,
        },
        shift::{g1_shift_params, g2_shift_params, ShiftG1ScalarMulProver, ShiftG2ScalarMulProver},
    },
    g2::{
        addition::{G2AddParams, G2AddProver, G2AddProverSpec},
        scalar_multiplication::{
            G2ScalarMulConstraintPolynomials, G2ScalarMulParams, G2ScalarMulProver,
            G2ScalarMulProverSpec, G2ScalarMulPublicInputs,
        },
    },
    gt::{
        claim_reduction::{GtExpClaimReductionParams, GtExpClaimReductionProver},
        exponentiation::{GtExpParams, GtExpProver, GtExpPublicInputs, GtExpWitness},
        multiplication::{GtMulConstraintPolynomials, GtMulParams, GtMulProver, GtMulProverSpec},
        shift::{GtShiftParams, GtShiftProver},
    },
    virtualization::extract_virtual_claims_from_accumulator,
};
use crate::poly::commitment::dory::{derive_from_dory_ast, recursion::JoltWitness};
use crate::poly::rlc_utils::compute_rlc_coefficients;
use crate::subprotocols::sumcheck::{BatchedSumcheck, SumcheckInstanceProof};
use crate::subprotocols::sumcheck_prover::SumcheckInstanceProver;
use crate::zkvm::guest_serde::{GuestDeserialize, GuestSerialize};
use crate::zkvm::recursion::prefix_packing::{packed_eval_from_claims, PrefixPackingLayout};
use crate::zkvm::recursion::witness::GTCombineWitness;

#[cfg(feature = "experimental-pairing-recursion")]
use super::pairing::{
    multi_miller_loop::{MultiMillerLoopParams, MultiMillerLoopProver, MultiMillerLoopProverSpec},
    shift::{ShiftMultiMillerLoopParams, ShiftMultiMillerLoopProver},
};

/// Proof generated by the recursion SNARK
#[derive(Clone, Debug, CanonicalSerialize, CanonicalDeserialize)]
pub struct RecursionProof<F: JoltField, T: Transcript, PCS: CommitmentScheme<Field = F>> {
    /// Stage 1: Packed GT exp sumcheck proof
    pub stage1_proof: SumcheckInstanceProof<F, T>,
    /// Stage 2: Batched constraint sumchecks proof.
    ///
    /// Includes:
    /// - packed-GT-exp internal consistency (shift rho)
    /// - packed-GT-exp claim reduction to a shared `r_x`
    /// - all other constraint families (GT mul, G1/G2 scalar mul, G1/G2 add, ...)
    pub stage2_proof: SumcheckInstanceProof<F, T>,
    /// Stage 3: Prefix-packed evaluation claim \(F(r_x \| r_{\mathrm{pack}})\).
    pub stage3_packed_eval: F,
    /// PCS opening proof for the constraint matrix
    pub opening_proof: PCS::Proof,
    /// Opening claims for virtual polynomials
    pub opening_claims: Openings<F>,
    /// Dense polynomial commitment after Stage 3 prefix packing.
    pub dense_commitment: PCS::Commitment,
}

impl<F, T, PCS> GuestSerialize for RecursionProof<F, T, PCS>
where
    F: JoltField + GuestSerialize,
    F::Challenge: GuestSerialize,
    PCS: CommitmentScheme<Field = F>,
    PCS::Proof: GuestSerialize,
    PCS::Commitment: GuestSerialize,
    T: Transcript,
{
    fn guest_serialize<W: std::io::Write>(&self, w: &mut W) -> std::io::Result<()> {
        self.stage1_proof.guest_serialize(w)?;
        self.stage2_proof.guest_serialize(w)?;
        self.stage3_packed_eval.guest_serialize(w)?;
        self.opening_proof.guest_serialize(w)?;
        self.opening_claims.guest_serialize(w)?;
        self.dense_commitment.guest_serialize(w)?;
        Ok(())
    }
}

impl<F, T, PCS> GuestDeserialize for RecursionProof<F, T, PCS>
where
    F: JoltField + GuestDeserialize,
    F::Challenge: GuestDeserialize,
    PCS: CommitmentScheme<Field = F>,
    PCS::Proof: GuestDeserialize,
    PCS::Commitment: GuestDeserialize,
    T: Transcript,
{
    fn guest_deserialize<R: std::io::Read>(r: &mut R) -> std::io::Result<Self> {
        Ok(Self {
            stage1_proof: SumcheckInstanceProof::<F, T>::guest_deserialize(r)?,
            stage2_proof: SumcheckInstanceProof::<F, T>::guest_deserialize(r)?,
            stage3_packed_eval: F::guest_deserialize(r)?,
            opening_proof: PCS::Proof::guest_deserialize(r)?,
            opening_claims: crate::poly::opening_proof::Openings::<F>::guest_deserialize(r)?,
            dense_commitment: PCS::Commitment::guest_deserialize(r)?,
        })
    }
}

/// Result type for recursion proof generation.
///
/// Contains the proof and constraint metadata needed by the verifier.
pub type RecursionProofResult<T, PCS> =
    Result<(RecursionProof<Fq, T, PCS>, RecursionConstraintMetadata), Box<dyn std::error::Error>>;

// -----------------------------------------------------------------------------
// Internal phase outputs (private helpers for RecursionProver refactor)
// -----------------------------------------------------------------------------

pub(crate) type HyraxPCS = Hyrax<1, GrumpkinProjective>;

pub(crate) struct HyraxPolyCommitPhaseOutput {
    pub(crate) metadata: RecursionConstraintMetadata,
    pub(crate) dense_commitment: HyraxCommitment<1, GrumpkinProjective>,
    pub(crate) dense_mlpoly: MultilinearPolynomial<Fq>,
}

pub(crate) struct SumcheckPhaseOutput<T: Transcript> {
    pub(crate) stage1_proof: SumcheckInstanceProof<Fq, T>,
    pub(crate) stage2_proof: SumcheckInstanceProof<Fq, T>,
    pub(crate) stage3_packed_eval: Fq,
    pub(crate) accumulator: ProverOpeningAccumulator<Fq>,
}

/// Unified prover for the recursion SNARK
#[derive(Clone)]
pub struct RecursionProver<F: JoltField = Fq> {
    /// The constraint system containing all constraints and witness data
    pub constraint_system: ConstraintSystem,
    /// AST graph for wiring constraints (optional, only present when using AST-enabled mode)
    pub ast: Option<AstGraph<BN254>>,
    /// Phantom for field type
    _marker: std::marker::PhantomData<F>,
}

/// Recursion-agnostic snapshot of the Stage 8 (Dory) opening state needed to kick off recursion proving.
///
/// This is produced by Stage 8 (batch opening proof) without performing any recursion-only work.
///
/// CRITICAL transcript invariant:
/// - `pre_opening_proof_transcript` must be forked from the main transcript **after** all Stage 8
///   claims have been appended and gamma has been sampled, but **before** `PCS::prove` mutates
///   the main transcript. This fork is what `PCS::witness_gen` must use (it mutates transcripts).
#[derive(Clone)]
pub struct DoryOpeningSnapshot<F: JoltField, ProofTranscript: Transcript> {
    /// Forked transcript state right after gamma sampling (before `PCS::prove`).
    pub pre_opening_proof_transcript: ProofTranscript,
    /// Unified opening point (big-endian challenges).
    pub opening_point: Vec<<F as JoltField>::Challenge>,
    /// Ordered (polynomial, claim) pairs used for Stage 8 RLC (with any embedding factors applied).
    ///
    /// Order matters: this must match the claim ordering used to sample gamma powers.
    pub polynomial_claims: Vec<(CommittedPolynomial, F)>,
    /// Gamma powers sampled from the transcript after appending all claims.
    pub gamma_powers: Vec<F>,
    /// Joint claim: Σ γ_i · claim_i.
    pub joint_claim: F,
}

/// Bundle of Stage 8 artifacts needed to start recursion proving.
///
/// This removes parameter sprawl at the Stage8→recursion boundary and makes it harder
/// to accidentally desynchronize proof/setup/snapshot inputs.
pub struct RecursionInput<'a, F: JoltField, PCS: RecursionExt<F>, ProofTranscript: Transcript> {
    pub stage8_opening_proof: &'a PCS::Proof,
    pub stage8_snapshot: DoryOpeningSnapshot<F, ProofTranscript>,
    pub verifier_setup: &'a PCS::VerifierSetup,
    pub commitments: &'a HashMap<CommittedPolynomial, PCS::Commitment>,
}

impl RecursionProver<Fq> {
    /// Phase 1: witness generation for recursion proving.
    ///
    /// Performs all recursion-only work needed to start recursion proving from the Stage 8 (Dory) opening:
    /// - combine witness generation (Stage 8 offload) + serialized `stage8_combine_hint`
    /// - `PCS::witness_gen_with_ast` (Stage 9) + symbolic AST capture
    /// - build the recursion constraint system and return `RecursionProver`
    #[allow(clippy::type_complexity)]
    #[tracing::instrument(skip_all, name = "RecursionProver::witness_generation")]
    fn witness_generation<F, PCS, ProofTranscript>(
        input: RecursionInput<'_, F, PCS, ProofTranscript>,
    ) -> Result<
        (Self, ark_bn254::Fq12, PCS::Ast, PairingBoundary, NonInputBaseHints),
        Box<dyn std::error::Error>,
    >
    where
        F: JoltField,
        PCS: RecursionExt<F, Witness = WitnessCollection<JoltWitness>, Ast = AstGraph<BN254>>,
        ProofTranscript: Transcript,
        PCS::Proof: 'static,
        PCS::VerifierSetup: 'static,
        PCS::Commitment: 'static,
        PCS::CombineHint: Send,
    {
        let stage8_opening_proof = input.stage8_opening_proof;
        let verifier_setup = input.verifier_setup;

        // Verify type compatibility at runtime.
        if std::any::TypeId::of::<F>() != std::any::TypeId::of::<ark_bn254::Fr>() {
            return Err(Box::new(std::io::Error::new(
                std::io::ErrorKind::InvalidInput,
                "Recursion SNARK requires F to be Fr (BN254 scalar field)",
            )));
        }

        // Stage 8 (recursion-only): compute joint commitment (value-only).
        let DoryOpeningSnapshot {
            pre_opening_proof_transcript,
            opening_point,
            polynomial_claims,
            gamma_powers,
            joint_claim,
        } = input.stage8_snapshot;

        let rlc_map = compute_rlc_coefficients(&gamma_powers, polynomial_claims);

        let mut coeffs = Vec::with_capacity(rlc_map.len());
        let mut comms = Vec::with_capacity(rlc_map.len());
        for (poly, coeff) in rlc_map.into_iter() {
            let commitment = input
                .commitments
                .get(&poly)
                .ok_or_else(|| format!("Missing commitment for Stage 8 polynomial {poly:?}"))?;
            coeffs.push(coeff);
            comms.push(commitment.clone());
        }

        let joint_commitment = PCS::combine_commitments(&comms, &coeffs);

        // Phase E3 (host/prover only): overlap combine-witness generation with the Stage 9
        // witness-gen (both are expensive but independent once `joint_commitment` is known).
        //
        // In verifier-only builds, keep sequential to avoid thread usage.
        let (witness_collection, ast, combine_witness, stage8_combine_hint_fq12) = {
            #[cfg(feature = "prover")]
            {
                let combine_out: Arc<Mutex<Option<(GTCombineWitness, PCS::CombineHint)>>> =
                    Arc::new(Mutex::new(None));

                let mut witness_out: Option<Result<(WitnessCollection<JoltWitness>, PCS::Ast), _>> =
                    None;

                rayon::scope(|s| {
                    // Spawn combine witness generation on the rayon threadpool.
                    let combine_out2 = combine_out.clone();
                    let comms_ref = &comms;
                    let coeffs_ref = &coeffs;
                    s.spawn(move |_| {
                        let _span = tracing::info_span!(
                            "stage8_generate_combine_witness",
                            num_commitments = comms_ref.len()
                        )
                        .entered();
                        let res = PCS::generate_combine_witness(comms_ref, coeffs_ref);
                        *combine_out2.lock().expect("combine_out mutex") = Some(res);
                    });

                    // Stage 9: use the PCS recursion extension to generate witnesses + AST for verifying
                    // the Stage 8 opening proof.
                    let mut witness_gen_transcript = pre_opening_proof_transcript;
                    let span = tracing::info_span!(
                        "stage9_witness_gen_with_ast",
                        opening_point_len = opening_point.len()
                    );
                    witness_out = Some(span.in_scope(|| {
                        PCS::witness_gen_with_ast(
                            stage8_opening_proof,
                            verifier_setup,
                            &mut witness_gen_transcript,
                            &opening_point,
                            &joint_claim,
                            &joint_commitment,
                        )
                    }));
                });

                let (witness_collection, ast) = witness_out
                    .expect("witness_out must be set in rayon scope")
                    .map_err(|e| Box::new(e) as Box<dyn std::error::Error>)?;

                let (combine_witness, combine_hint) = combine_out
                    .lock()
                    .expect("combine_out mutex")
                    .take()
                    .expect("combine witness must be produced");
                let stage8_combine_hint_fq12 = PCS::combine_hint_to_fq12(&combine_hint);

                (
                    witness_collection,
                    ast,
                    combine_witness,
                    stage8_combine_hint_fq12,
                )
            }
            #[cfg(not(feature = "prover"))]
            {
                let (combine_witness, combine_hint) =
                    PCS::generate_combine_witness(&comms, &coeffs);
                let stage8_combine_hint_fq12 = PCS::combine_hint_to_fq12(&combine_hint);

                let mut witness_gen_transcript = pre_opening_proof_transcript;
                let (witness_collection, ast) = tracing::info_span!(
                    "stage9_witness_gen_with_ast",
                    opening_point_len = opening_point.len()
                )
                .in_scope(|| {
                    PCS::witness_gen_with_ast(
                        stage8_opening_proof,
                        verifier_setup,
                        &mut witness_gen_transcript,
                        &opening_point,
                        &joint_claim,
                        &joint_commitment,
                    )
                })?;

                (
                    witness_collection,
                    ast,
                    combine_witness,
                    stage8_combine_hint_fq12,
                )
            }
        };

        // Phase F (pairing boundary): derive boundary from the (fully realized) AST and Stage 8 inputs.
        //
        // This is later re-derived by the verifier from a symbolic AST for binding.
        use std::any::Any;

        // `coeffs` are sampled over the Stage-8 transcript field `F`.
        // In recursion mode we require `F = ark_bn254::Fr` (enforced above), so avoid
        // serialize/deserialize round-trips which are extremely expensive for large batches.
        let combine_coeffs_fr: Vec<Fr> = coeffs
            .iter()
            .map(|c| {
                (c as &dyn Any)
                    .downcast_ref::<Fr>()
                    .copied()
                    .ok_or(ark_serialize::SerializationError::InvalidData)
            })
            .collect::<Result<_, ark_serialize::SerializationError>>()?;

        let dory_proof = (stage8_opening_proof as &dyn Any)
            .downcast_ref::<ArkDoryProof>()
            .ok_or_else(|| {
                std::io::Error::new(
                    std::io::ErrorKind::InvalidInput,
                    "Recursion SNARK requires Dory proof type",
                )
            })?;
        let dory_setup = (verifier_setup as &dyn Any)
            .downcast_ref::<ArkworksVerifierSetup>()
            .ok_or_else(|| {
                std::io::Error::new(
                    std::io::ErrorKind::InvalidInput,
                    "Recursion SNARK requires Dory verifier setup type",
                )
            })?;
        let joint_commitment_dory = (&joint_commitment as &dyn Any)
            .downcast_ref::<ArkGT>()
            .cloned()
            .ok_or_else(|| {
                std::io::Error::new(
                    std::io::ErrorKind::InvalidInput,
                    "Recursion SNARK requires Dory commitment type",
                )
            })?;
        let comms_dory: Vec<ArkGT> = comms
            .iter()
            .map(|c| {
                (c as &dyn Any)
                    .downcast_ref::<ArkGT>()
                    .cloned()
                    .ok_or_else(|| {
                        std::io::Error::new(
                            std::io::ErrorKind::InvalidInput,
                            "Recursion SNARK requires Dory commitment type",
                        )
                    })
            })
            .collect::<Result<_, _>>()?;

        let derived = derive_from_dory_ast(
            &ast,
            dory_proof,
            dory_setup,
            joint_commitment_dory,
            &comms_dory,
            &combine_coeffs_fr,
        )
        .map_err(|_e| {
            std::io::Error::new(
                std::io::ErrorKind::InvalidData,
                "AST->pairing-boundary derivation failed",
            )
        })?;

        // Build constraint system from generated witnesses and include combine witness constraints.
        let prover = Self::new_from_witnesses(&witness_collection, Some(combine_witness))?;

        // Non-input base/point hints for verifier-side instance-plan derivation (perf-only until wiring is added).
        let non_input_base_hints = tracing::info_span!("derive_non_input_base_hints").in_scope(|| {
            use dory::recursion::ast::AstOp;
            use dory::recursion::OpId;

            // Collect op lists in OpId order (must match verifier derivation).
            let mut gt_exp: Vec<(OpId, dory::recursion::ast::ValueId)> = Vec::new();
            let mut g1_smul: Vec<(OpId, dory::recursion::ast::ValueId)> = Vec::new();
            let mut g2_smul: Vec<(OpId, dory::recursion::ast::ValueId)> = Vec::new();
            for node in &ast.nodes {
                match &node.op {
                    AstOp::GTExp {
                        op_id: Some(id),
                        base,
                        ..
                    } => gt_exp.push((*id, *base)),
                    AstOp::G1ScalarMul {
                        op_id: Some(id),
                        point,
                        ..
                    } => g1_smul.push((*id, *point)),
                    AstOp::G2ScalarMul {
                        op_id: Some(id),
                        point,
                        ..
                    } => g2_smul.push((*id, *point)),
                    _ => {}
                }
            }
            gt_exp.sort_by_key(|(id, _)| *id);
            g1_smul.sort_by_key(|(id, _)| *id);
            g2_smul.sort_by_key(|(id, _)| *id);

            let is_input = |vid: dory::recursion::ast::ValueId| -> bool {
                let idx = vid.0 as usize;
                idx < ast.nodes.len() && matches!(ast.nodes[idx].op, AstOp::Input { .. })
            };

            let gt_exp_base_hints = gt_exp
                .iter()
                .map(|(op_id, base_id)| {
                    if is_input(*base_id) {
                        None
                    } else {
                        Some(
                            witness_collection
                                .gt_exp
                                .get(op_id)
                                .expect("missing GTExp witness for op_id")
                                .base,
                        )
                    }
                })
                .collect();
            let g1_scalar_mul_base_hints = g1_smul
                .iter()
                .map(|(op_id, point_id)| {
                    if is_input(*point_id) {
                        None
                    } else {
                        Some(
                            witness_collection
                                .g1_scalar_mul
                                .get(op_id)
                                .expect("missing G1ScalarMul witness for op_id")
                                .point_base,
                        )
                    }
                })
                .collect();
            let g2_scalar_mul_base_hints = g2_smul
                .iter()
                .map(|(op_id, point_id)| {
                    if is_input(*point_id) {
                        None
                    } else {
                        Some(
                            witness_collection
                                .g2_scalar_mul
                                .get(op_id)
                                .expect("missing G2ScalarMul witness for op_id")
                                .point_base,
                        )
                    }
                })
                .collect();

            NonInputBaseHints {
                gt_exp_base_hints,
                g1_scalar_mul_base_hints,
                g2_scalar_mul_base_hints,
            }
        });

        Ok((
            prover,
            stage8_combine_hint_fq12,
            ast,
            derived.pairing_boundary,
            non_input_base_hints,
        ))
    }

    /// Full Stage8→recursion pipeline entrypoint.
    ///
    /// This is the definitive call for recursion proving:
    /// 1) `witness_generation` (Stage 8 offload + Stage 9 `witness_gen` + build CS)\n
    /// 2) `poly_commit` (Hyrax commit)\n
    /// 3) `prove_sumchecks` (all recursion sumchecks)\n
    /// 4) `poly_opening` (Hyrax opening)\n
    ///
    /// Returns the recursion proof and internal verifier metadata.
    #[allow(clippy::type_complexity)]
    pub fn prove<F, DoryPCS, ProofTranscript>(
        transcript: &mut ProofTranscript,
        hyrax_prover_setup: &PedersenGenerators<GrumpkinProjective>,
        input: RecursionInput<'_, F, DoryPCS, ProofTranscript>,
    ) -> Result<
        (
            RecursionProof<Fq, ProofTranscript, HyraxPCS>,
            RecursionConstraintMetadata,
            PairingBoundary,
            Option<ark_bn254::Fq12>,
            NonInputBaseHints,
        ),
        Box<dyn std::error::Error>,
    >
    where
        F: JoltField,
        DoryPCS: RecursionExt<F, Witness = WitnessCollection<JoltWitness>, Ast = AstGraph<BN254>>,
        DoryPCS::CombineHint: Send,
        ProofTranscript: Transcript,
    {
        // Phase 1: witness generation
        let (mut prover, stage8_combine_hint_fq12, ast, pairing_boundary, non_input_base_hints) =
            Self::witness_generation::<F, DoryPCS, ProofTranscript>(input)?;
        prover.ast = Some(ast);

        // Phase 2: polynomial commitment (Hyrax)
        let poly_commit = prover.poly_commit::<ProofTranscript>(transcript, hyrax_prover_setup)?;

        // Phase 3: sumchecks
        let sumchecks =
            prover.prove_sumchecks::<ProofTranscript>(transcript, &poly_commit.metadata)?;

        // Phase 4: polynomial opening (Hyrax)
        let (opening_proof, opening_claims) = Self::poly_opening::<ProofTranscript, HyraxPCS>(
            transcript,
            hyrax_prover_setup,
            sumchecks.accumulator,
            poly_commit.dense_mlpoly,
        )?;

        let proof = RecursionProof {
            stage1_proof: sumchecks.stage1_proof,
            stage2_proof: sumchecks.stage2_proof,
            stage3_packed_eval: sumchecks.stage3_packed_eval,
            opening_proof,
            opening_claims,
            dense_commitment: poly_commit.dense_commitment,
        };

        Ok((
            proof,
            poly_commit.metadata,
            pairing_boundary,
            Some(stage8_combine_hint_fq12),
            non_input_base_hints,
        ))
    }

    /// Create a new recursion prover from pre-generated witnesses
    pub fn new_from_witnesses(
        witness_collection: &WitnessCollection<JoltWitness>,
        combine_witness: Option<GTCombineWitness>,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        tracing::debug!(
            "Creating RecursionProver from witnesses: GT exp count = {}, GT mul count = {}",
            witness_collection.gt_exp.len(),
            witness_collection.gt_mul.len()
        );

        // Compute g_poly directly (was previously computed in witnesses_to_dory_recursion)
        // g(x) is the irreducible polynomial defining the Fq12 extension field
        let g_poly = {
            let g_mle_4var = get_g_mle();
            // Keep `g(x)` in its native 4-var form (size 16). Any embeddings are handled
            // locally by the specific constraint family.
            DensePolynomial::new(g_mle_4var)
        };

        // Build constraint system from witness collection using DoryMatrixBuilder
        let build_cs_span = tracing::info_span!("build_constraint_system").entered();
        let constraint_system =
            Self::build_constraint_system(witness_collection, combine_witness.as_ref(), g_poly)?;
        drop(build_cs_span);

        Ok(Self {
            constraint_system,
            ast: None,
            _marker: std::marker::PhantomData,
        })
    }

    /// Create a new recursion prover by generating witnesses from a Dory proof
    pub fn new_from_dory_proof<T: Transcript>(
        dory_proof: &ArkDoryProof,
        verifier_setup: &ArkworksVerifierSetup,
        transcript: &mut T,
        point: &[<Fr as JoltField>::Challenge],
        evaluation: &Fr,
        commitment: &ArkGT,
    ) -> Result<Self, Box<dyn std::error::Error>> {
        // Use Dory's witness_gen to generate witnesses
        let (witness_collection, _ast) = DoryCommitmentScheme::witness_gen_with_ast(
            dory_proof,
            verifier_setup,
            transcript,
            point,
            evaluation,
            commitment,
        )?;

        // Delegate to new_from_witnesses (no combine_witness from direct Dory proof)
        Self::new_from_witnesses(&witness_collection, None)
    }

    /// Create a new recursion prover with AST tracing enabled.
    ///
    /// This captures the full computation DAG, enabling:
    /// Build constraint system from recursion witness
    fn build_constraint_system(
        witness_collection: &WitnessCollection<JoltWitness>,
        combine_witness: Option<&GTCombineWitness>,
        g_poly: DensePolynomial<Fq>,
    ) -> Result<ConstraintSystem, Box<dyn std::error::Error>> {
        // Constraint-family toggles (useful to isolate failures).
        // Defaults enable the full constraint set; disable families by setting env vars to "0".
        let env_flag_default = |name: &str, default: bool| -> bool {
            std::env::var(name)
                .ok()
                .map(|v| v != "0" && v.to_lowercase() != "false")
                .unwrap_or(default)
        };
        let enable_gt_mul = env_flag_default("JOLT_RECURSION_ENABLE_GT_MUL", true);
        let enable_g1_scalar_mul = env_flag_default("JOLT_RECURSION_ENABLE_G1_SCALAR_MUL", true);
        let enable_g2_scalar_mul = env_flag_default("JOLT_RECURSION_ENABLE_G2_SCALAR_MUL", true);
        let enable_g1_add = env_flag_default("JOLT_RECURSION_ENABLE_G1_ADD", true);
        let enable_g2_add = env_flag_default("JOLT_RECURSION_ENABLE_G2_ADD", true);
        #[cfg(feature = "experimental-pairing-recursion")]
        let enable_pairing = env_flag_default("JOLT_RECURSION_ENABLE_PAIRING", true);

        // Calculate expected constraint count for pre-allocation
        let expected_constraints = witness_collection.gt_exp.len()
            + witness_collection.gt_mul.len()
            + witness_collection.g1_scalar_mul.len()
            + witness_collection.g2_scalar_mul.len()
            + witness_collection.g1_add.len()
            + witness_collection.g2_add.len()
            + combine_witness.map_or(0, |cw| {
                cw.exp_witnesses.len() + cw.mul_layers.iter().map(|l| l.len()).sum::<usize>()
            });

        // Use DoryMatrixBuilder with 11 variables and pre-allocated capacity
        let mut builder = DoryMatrixBuilder::with_capacity(11, expected_constraints);

        // Build packed GT exp witnesses and add to matrix
        tracing::info!(
            "[build_constraint_system] Processing {} direct GT exp witnesses",
            witness_collection.gt_exp.len()
        );
        let gt_exp_span = tracing::info_span!(
            "build_gt_exp_witnesses",
            count = witness_collection.gt_exp.len()
        )
        .entered();
        let mut g1_scalar_mul_public_inputs =
            Vec::with_capacity(witness_collection.g1_scalar_mul.len());
        let mut g2_scalar_mul_public_inputs =
            Vec::with_capacity(witness_collection.g2_scalar_mul.len());

        // Sort once, then parallelize expensive witness preparation
        let mut gt_exp_items: Vec<_> = witness_collection.gt_exp.iter().collect();
        gt_exp_items.sort_by_key(|(op_id, _)| *op_id);

        // Parallel computation of packed witnesses (expensive work)
        let prepared_gt_exp: Vec<_> = gt_exp_items
            .par_iter()
            .map(|(_op_id, witness)| {
                // Convert base ArkGT to 4-var MLE (expensive)
                let base_mle = fq12_to_multilinear_evals(&witness.base);
                let base2 = witness.base * witness.base;
                let base2_mle = fq12_to_multilinear_evals(&base2);
                let base3 = base2 * witness.base;
                let base3_mle = fq12_to_multilinear_evals(&base3);

                // Create packed witness (expensive)
                let packed = GtExpWitness::from_steps(
                    &witness.rho_mles,
                    &witness.quotient_mles,
                    &witness.bits,
                    &base_mle,
                    &base2_mle,
                    &base3_mle,
                );

                let public_input = GtExpPublicInputs::new(witness.base, witness.bits.clone());
                (packed, public_input)
            })
            .collect();

        // Sequential addition to builder (requires mutable state)
        let mut gt_exp_witnesses = Vec::with_capacity(prepared_gt_exp.len());
        let mut gt_exp_public_inputs = Vec::with_capacity(prepared_gt_exp.len());
        for (packed, public_input) in prepared_gt_exp {
            builder.add_gt_exp_witness(&packed);
            gt_exp_witnesses.push(packed);
            gt_exp_public_inputs.push(public_input);
        }
        drop(gt_exp_span);

        // Add GT mul witnesses (optional) - parallelized preparation
        if enable_gt_mul {
            let gt_mul_span = tracing::info_span!(
                "add_gt_mul_witnesses",
                count = witness_collection.gt_mul.len()
            )
            .entered();
            let mut gt_mul_items: Vec<_> = witness_collection.gt_mul.iter().collect();
            gt_mul_items.sort_by_key(|(op_id, _)| *op_id);

            // Parallel preparation of GT mul witnesses (expensive fq12_to_multilinear_evals)
            let num_constraint_vars = 11; // Same as builder
            let prepared_gt_mul: Vec<_> = gt_mul_items
                .par_iter()
                .map(|(_op_id, witness)| {
                    DoryMatrixBuilder::prepare_gt_mul_witness(witness, num_constraint_vars)
                })
                .collect();

            // Sequential addition to builder (requires mutable state)
            for prepared in prepared_gt_mul {
                builder.add_prepared_gt_mul_witness(prepared);
            }
            drop(gt_mul_span);
        }

        // Add G1 scalar mul witnesses (optional)
        if enable_g1_scalar_mul {
            let g1_scalar_mul_span = tracing::info_span!(
                "add_g1_scalar_mul_witnesses",
                count = witness_collection.g1_scalar_mul.len()
            )
            .entered();
            let mut g1_items: Vec<_> = witness_collection.g1_scalar_mul.iter().collect();
            g1_items.sort_by_key(|(op_id, _)| *op_id);
            for (_op_id, witness) in g1_items {
                builder.add_g1_scalar_mul_witness(witness);
                g1_scalar_mul_public_inputs.push(G1ScalarMulPublicInputs::new(witness.scalar));
            }
            drop(g1_scalar_mul_span);
        }

        // Add G2 scalar mul witnesses (optional)
        if enable_g2_scalar_mul {
            let g2_scalar_mul_span = tracing::info_span!(
                "add_g2_scalar_mul_witnesses",
                count = witness_collection.g2_scalar_mul.len()
            )
            .entered();
            let mut g2_items: Vec<_> = witness_collection.g2_scalar_mul.iter().collect();
            g2_items.sort_by_key(|(op_id, _)| *op_id);
            for (_op_id, witness) in g2_items {
                builder.add_g2_scalar_mul_witness(witness);
                g2_scalar_mul_public_inputs.push(G2ScalarMulPublicInputs::new(witness.scalar));
            }
            drop(g2_scalar_mul_span);
        }

        // Add G1 add witnesses (optional; into the matrix)
        if enable_g1_add {
            let g1_add_span = tracing::info_span!(
                "add_g1_add_witnesses",
                count = witness_collection.g1_add.len()
            )
            .entered();
            let mut g1_add_items: Vec<_> = witness_collection.g1_add.iter().collect();
            g1_add_items.sort_by_key(|(op_id, _)| *op_id);
            for (_op_id, witness) in g1_add_items {
                builder.add_g1_add_witness(witness);
            }
            drop(g1_add_span);
        }

        // Add G2 add witnesses (optional; into the matrix)
        if enable_g2_add {
            let g2_add_span = tracing::info_span!(
                "add_g2_add_witnesses",
                count = witness_collection.g2_add.len()
            )
            .entered();
            let mut g2_add_items: Vec<_> = witness_collection.g2_add.iter().collect();
            g2_add_items.sort_by_key(|(op_id, _)| *op_id);
            for (_op_id, witness) in g2_add_items {
                builder.add_g2_add_witness(witness);
            }
            drop(g2_add_span);
        }

        // Add pairing (Multi-Miller loop) witnesses (experimental; into the matrix)
        #[cfg(feature = "experimental-pairing-recursion")]
        if enable_pairing {
            let pairing_span = tracing::info_span!(
                "add_pairing_witnesses",
                count = witness_collection.pairing.len(),
                multi_count = witness_collection.multi_pairing.len()
            )
            .entered();

            let mut pairing_items: Vec<_> = witness_collection.pairing.iter().collect();
            pairing_items.sort_by_key(|(op_id, _)| *op_id);
            for (_op_id, witness) in pairing_items {
                builder.add_multi_miller_loop_witness(witness);
            }

            let mut multi_pairing_items: Vec<_> = witness_collection.multi_pairing.iter().collect();
            multi_pairing_items.sort_by_key(|(op_id, _)| *op_id);
            for (_op_id, witness) in multi_pairing_items {
                builder.add_multi_miller_loop_witness(witness);
            }

            drop(pairing_span);
        }

        // Add combine_commitments witnesses (homomorphic combine offloading)
        if let Some(cw) = combine_witness {
            let combine_span = tracing::info_span!(
                "add_homomorphic_combine_witnesses",
                gt_exp_count = cw.exp_witnesses.len(),
                gt_mul_count = cw.mul_layers.iter().map(|l| l.len()).sum::<usize>()
            )
            .entered();

            let pre_count = builder.constraint_count();
            tracing::info!(
                "[Homomorphic Combine] Adding {} GT exp + {} GT mul constraints (pre-count: {})",
                cw.exp_witnesses.len(),
                cw.mul_layers.iter().map(|l| l.len()).sum::<usize>(),
                pre_count
            );

            // Also collect public inputs for the combine witnesses
            for exp_wit in &cw.exp_witnesses {
                gt_exp_public_inputs
                    .push(GtExpPublicInputs::new(exp_wit.base, exp_wit.bits.clone()));
            }

            let combined_packed_witnesses = builder.add_combine_witness(cw);
            tracing::info!(
                "[Homomorphic Combine] Post-add constraint count: {}, packed witnesses: {}",
                builder.constraint_count(),
                combined_packed_witnesses.len()
            );
            // Add the combined witnesses to our list
            tracing::info!(
                "[Homomorphic Combine] Before extend: {} witnesses, adding {} more",
                gt_exp_witnesses.len(),
                combined_packed_witnesses.len()
            );
            gt_exp_witnesses.extend(combined_packed_witnesses);
            drop(combine_span);
        }

        let build_matrix_span = tracing::info_span!("build_matrix").entered();
        let (matrix, constraints) = builder.build();
        drop(build_matrix_span);

        Ok(ConstraintSystem {
            constraints,
            matrix,
            g_poly,
            gt_exp_witnesses,
            gt_exp_public_inputs,
            g1_scalar_mul_public_inputs,
            g2_scalar_mul_public_inputs,
            // G1/G2 add constraints are stored in the matrix; Stage 1 extracts them from the matrix.
            g1_add_witnesses: Vec::new(),
            g2_add_witnesses: Vec::new(),
        })
    }
}

impl RecursionProver<Fq> {
    #[tracing::instrument(skip_all, name = "RecursionProver::poly_commit")]
    pub(crate) fn poly_commit<T: Transcript>(
        &mut self,
        transcript: &mut T,
        prover_setup: &PedersenGenerators<GrumpkinProjective>,
    ) -> Result<HyraxPolyCommitPhaseOutput, Box<dyn std::error::Error>> {
        // ============ BUILD PREFIX-PACKED DENSE POLYNOMIAL ============
        // IMPORTANT: Commitment must happen BEFORE sumchecks for soundness!
        let (dense_poly, prefix_layout) = tracing::info_span!("build_prefix_packed_polynomial")
            .in_scope(|| {
                tracing::info!("Building prefix-packed dense polynomial");
                self.constraint_system.build_prefix_packed_polynomial()
            });

        let dense_num_vars = prefix_layout.num_dense_vars;
        let unpadded_len = prefix_layout.unpadded_len();
        let padded_len = dense_poly.len();

        // Log how much Hyrax work we skip by avoiding padded zeros.
        let (l_size, r_size) = matrix_dimensions(dense_num_vars, 1);
        debug_assert_eq!(l_size * r_size, padded_len);
        let used_rows = unpadded_len.div_ceil(r_size);
        let skipped_rows = l_size.saturating_sub(used_rows);
        tracing::info!(
            dense_num_vars,
            unpadded_len,
            padded_len,
            r_size,
            l_size,
            used_rows,
            skipped_rows,
            tail_elems = (unpadded_len % r_size),
            "Hyrax commit skipping zero-padded tail"
        );

        // ============ BUILD METADATA (for verifier) ============
        let metadata = tracing::info_span!("build_constraint_metadata").in_scope(|| {
            tracing::info!("Building constraint metadata for verifier");
            let constraint_types: Vec<_> = self
                .constraint_system
                .constraints
                .iter()
                .map(|c| c.constraint_type.clone())
                .collect();

            RecursionConstraintMetadata {
                constraint_types,
                dense_num_vars,
                gt_exp_public_inputs: self.constraint_system.gt_exp_public_inputs.clone(),
                g1_scalar_mul_public_inputs: self
                    .constraint_system
                    .g1_scalar_mul_public_inputs
                    .clone(),
                g2_scalar_mul_public_inputs: self
                    .constraint_system
                    .g2_scalar_mul_public_inputs
                    .clone(),
            }
        });

        // Hyrax commit (optimized): avoid MSM work on zero-padded suffix.
        let dense_commitment = HyraxCommitment::<1, GrumpkinProjective>::commit_with_unpadded_len(
            &dense_poly,
            prover_setup,
            unpadded_len,
        );

        // Convert to multilinear polynomial for downstream sumchecks / opening proof.
        let dense_mlpoly = MultilinearPolynomial::from(dense_poly.Z);
        tracing::info!(
            "Hyrax commitment terms: {}, unpadded_len: {}",
            dense_mlpoly.len(),
            unpadded_len
        );

        // Add commitment to transcript for Fiat-Shamir soundness
        transcript.append_serializable(&dense_commitment);

        Ok(HyraxPolyCommitPhaseOutput {
            metadata,
            dense_commitment,
            dense_mlpoly,
        })
    }

    #[tracing::instrument(skip_all, name = "RecursionProver::prove_sumchecks")]
    pub(crate) fn prove_sumchecks<T: Transcript>(
        &mut self,
        transcript: &mut T,
        metadata: &RecursionConstraintMetadata,
    ) -> Result<SumcheckPhaseOutput<T>, Box<dyn std::error::Error>> {
        // ============ RUN ALL SUMCHECKS ============
        // Initialize opening accumulator
        let log_T = self.constraint_system.num_vars();
        let mut accumulator = tracing::info_span!("init_opening_accumulator").in_scope(|| {
            let acc = ProverOpeningAccumulator::<Fq>::new(log_T);
            tracing::info!("Initialized opening accumulator with {} variables", log_T);
            acc
        });

        // Stage 1: Packed GT exp sumcheck
        let (stage1_proof, _r_stage1_packed) = self
            .prove_stage1(transcript, &mut accumulator)
            .expect("Failed to run Stage 1 (GtExp)");

        // Stage 2: Batched constraint sumchecks
        let (stage2_proof, r_stage2) = self
            .prove_stage2(transcript, &mut accumulator)
            .expect("Failed to run Stage 2 (constraint sumchecks)");

        // Stage 2 challenges layout:
        // - legacy: r_stage2 = r_x (length = num_constraint_vars)
        // - fused:  r_stage2 includes extra index variables `r_c` (length >= num_constraint_vars + k)
        //
        // NOTE: Recursion constraint sumchecks are **suffix-aligned** in the batched sumcheck
        // (`round_offset = max_num_rounds - num_rounds`), so shorter points are suffixes of longer
        // ones in the batched challenge order. We therefore interpret `r_x` as the **suffix** of
        // the Stage-2 challenge vector.
        let k = self.constraint_system.matrix.num_constraint_index_vars;
        let num_constraint_vars = self.constraint_system.matrix.num_constraint_vars;
        if r_stage2.len() != num_constraint_vars && r_stage2.len() < num_constraint_vars + k {
            return Err(format!(
                "Stage 2 returned {} challenges, expected {} (legacy) or at least {} (num_constraint_vars + k)",
                r_stage2.len(),
                num_constraint_vars,
                num_constraint_vars + k
            )
            .into());
        }
        let (_r_c, r_x): (
            &[<Fq as JoltField>::Challenge],
            &[<Fq as JoltField>::Challenge],
        ) = if r_stage2.len() == num_constraint_vars {
            (&[], &r_stage2)
        } else {
            let r_x_start = r_stage2.len() - num_constraint_vars;
            let r_x = &r_stage2[r_x_start..];
            let r_c = &r_stage2[r_x_start - k..r_x_start];
            (r_c, r_x)
        };

        // Stage 3: Prefix packing (direct reduction to a single Hyrax opening)
        let stage3_packed_eval = self
            .prove_stage3_prefix_packing(transcript, &mut accumulator, metadata, r_x)
            .expect("Failed to run Stage 3 (prefix packing)");

        Ok(SumcheckPhaseOutput {
            stage1_proof,
            stage2_proof,
            stage3_packed_eval,
            accumulator,
        })
    }

    #[tracing::instrument(skip_all, name = "RecursionProver::poly_opening")]
    pub(crate) fn poly_opening<T: Transcript, PCS: CommitmentScheme<Field = Fq>>(
        transcript: &mut T,
        prover_setup: &PCS::ProverSetup,
        mut accumulator: ProverOpeningAccumulator<Fq>,
        dense_mlpoly: MultilinearPolynomial<Fq>,
    ) -> Result<(PCS::Proof, Openings<Fq>), Box<dyn std::error::Error>> {
        tracing::info!("Generating PCS opening proof");

        // Create polynomial map for opening proof
        let mut polynomials_map: HashMap<CommittedPolynomial, MultilinearPolynomial<Fq>> =
            HashMap::new();
        polynomials_map.insert(CommittedPolynomial::DoryDenseMatrix, dense_mlpoly);

        // Generate opening proof using PCS
        let opening_proof = accumulator
            .prove_single::<T, PCS>(polynomials_map, prover_setup, transcript)
            .expect("Failed to generate PCS opening proof");

        let opening_claims = accumulator.openings.clone();
        tracing::info!(
            "Generated opening proof with {} claims",
            opening_claims.len()
        );

        Ok((opening_proof, opening_claims))
    }

    /// Run Stage 1: Packed GT exp sumcheck
    #[allow(clippy::type_complexity)]
    #[tracing::instrument(skip_all, name = "RecursionProver::prove_stage1")]
    pub(crate) fn prove_stage1<T: Transcript>(
        &self,
        transcript: &mut T,
        accumulator: &mut ProverOpeningAccumulator<Fq>,
    ) -> Result<
        (
            SumcheckInstanceProof<Fq, T>,
            Vec<<Fq as JoltField>::Challenge>,
        ),
        Box<dyn std::error::Error>,
    > {
        let packed_witnesses = &self.constraint_system.gt_exp_witnesses;
        if packed_witnesses.is_empty() {
            return Err("No GtExp constraints to prove in Stage 1".into());
        }

        // Packed GT exp uses layout x * 128 + s (s in low bits), so g needs replication
        // across the step variables.
        let g_4var: Vec<Fq> = self.constraint_system.g_poly.evals()[0..16].to_vec();
        let g_replicated = DoryMatrixBuilder::pad_4var_to_11var_replicated(&g_4var);
        let g_poly_replicated_f = DensePolynomial::new(g_replicated);

        let params = GtExpParams::new();
        tracing::info!(
            "[Stage 1] Creating GtExpProver with {} witnesses",
            packed_witnesses.len()
        );
        let mut packed_gt_exp_prover =
            GtExpProver::new(params, packed_witnesses, g_poly_replicated_f, transcript);

        // Run the (single-instance) sumcheck.
        let (proof, r_stage1) =
            BatchedSumcheck::prove(vec![&mut packed_gt_exp_prover], accumulator, transcript);

        Ok((proof, r_stage1))
    }

    /// Run Stage 2: Batched constraint sumchecks.
    ///
    /// This stage includes:
    /// - packed-GT-exp internal consistency (`GtShift`)
    /// - packed-GT-exp claim reduction to a shared `r_x`
    /// - all remaining constraint families (GT mul, G1/G2 scalar mul, G1/G2 add, ...)
    #[allow(clippy::type_complexity)]
    #[tracing::instrument(skip_all, name = "RecursionProver::prove_stage2")]
    pub(crate) fn prove_stage2<T: Transcript>(
        &self,
        transcript: &mut T,
        accumulator: &mut ProverOpeningAccumulator<Fq>,
    ) -> Result<
        (
            SumcheckInstanceProof<Fq, T>,
            Vec<<Fq as JoltField>::Challenge>, // r_stage2 = [r_c || r_x]
        ),
        Box<dyn std::error::Error>,
    > {
        let env_flag_default = |name: &str, default: bool| -> bool {
            std::env::var(name)
                .ok()
                .map(|v| v != "0" && v.to_lowercase() != "false")
                .unwrap_or(default)
        };
        let enable_shift_rho = env_flag_default("JOLT_RECURSION_ENABLE_SHIFT_RHO", true);
        let enable_shift_g1_scalar_mul =
            env_flag_default("JOLT_RECURSION_ENABLE_SHIFT_G1_SCALAR_MUL", true);
        let enable_shift_g2_scalar_mul =
            env_flag_default("JOLT_RECURSION_ENABLE_SHIFT_G2_SCALAR_MUL", true);
        let enable_claim_reduction = env_flag_default("JOLT_RECURSION_ENABLE_PGX_REDUCTION", true);
        #[cfg(feature = "experimental-pairing-recursion")]
        let enable_shift_multi_miller_loop =
            env_flag_default("JOLT_RECURSION_ENABLE_SHIFT_MULTI_MILLER_LOOP", true);

        let mut provers: Vec<Box<dyn SumcheckInstanceProver<Fq, T>>> = Vec::new();

        // ---- Packed GT exp (shift rho + claim reduction) ----
        let num_gt_exp = self.constraint_system.gt_exp_witnesses.len();
        if num_gt_exp > 0 {
            let claim_indices: Vec<usize> = (0..num_gt_exp).collect();

            if enable_shift_rho {
                let rho_polys: Vec<Vec<Fq>> = self
                    .constraint_system
                    .gt_exp_witnesses
                    .iter()
                    .map(|w| w.rho_packed.clone())
                    .collect();
                let shift_params = GtShiftParams::new(num_gt_exp);
                let shift_prover = GtShiftProver::<Fq, T>::new(
                    shift_params,
                    rho_polys,
                    claim_indices.clone(),
                    accumulator,
                    transcript,
                );
                provers.push(Box::new(shift_prover));
            }

            if enable_claim_reduction {
                let rho_polys: Vec<MultilinearPolynomial<Fq>> = self
                    .constraint_system
                    .gt_exp_witnesses
                    .iter()
                    .map(|w| MultilinearPolynomial::from(w.rho_packed.clone()))
                    .collect();
                let quotient_polys: Vec<MultilinearPolynomial<Fq>> = self
                    .constraint_system
                    .gt_exp_witnesses
                    .iter()
                    .map(|w| MultilinearPolynomial::from(w.quotient_packed.clone()))
                    .collect();
                let reduction_params = GtExpClaimReductionParams::new(2 * num_gt_exp);
                let reduction_prover = GtExpClaimReductionProver::<Fq, T>::new(
                    reduction_params,
                    &claim_indices,
                    rho_polys,
                    quotient_polys,
                    accumulator,
                    transcript,
                );
                provers.push(Box::new(reduction_prover));
            }
        }

        // ---- GT mul / G1+G2 scalar mul / G1+G2 add (and any other Stage 2 constraints) ----
        //
        // NOTE: These were previously run in Stage 1; we keep their logic but batch them here.

        // Convert g_poly for GT mul (uses zero padding layout: s * 16 + x)
        let g_poly_f = self.constraint_system.g_poly.clone();

        // GT mul
        let gt_mul_constraints_tuples = self.constraint_system.extract_gt_mul_constraints();
        if !gt_mul_constraints_tuples.is_empty() {
            let constraint_indices: Vec<usize> = (0..gt_mul_constraints_tuples.len()).collect();
            let gt_mul_constraints_fq: Vec<GtMulConstraintPolynomials<Fq>> =
                gt_mul_constraints_tuples
                    .into_iter()
                    .map(
                        |(idx, lhs, rhs, result, quotient)| GtMulConstraintPolynomials {
                            lhs,
                            rhs,
                            result,
                            quotient,
                            constraint_index: idx,
                        },
                    )
                    .collect();

            let params = GtMulParams::new(gt_mul_constraints_fq.len());
            let spec = GtMulProverSpec::new(params, gt_mul_constraints_fq, g_poly_f.clone());
            let prover = GtMulProver::from_spec(spec, constraint_indices, transcript);
            provers.push(Box::new(prover));
        }

        // G1 scalar mul
        let g1_scalar_mul_constraints_tuples =
            self.constraint_system.extract_g1_scalar_mul_constraints();
        if !g1_scalar_mul_constraints_tuples.is_empty() {
            debug_assert_eq!(
                self.constraint_system.g1_scalar_mul_public_inputs.len(),
                g1_scalar_mul_constraints_tuples.len(),
                "ConstraintSystem.g1_scalar_mul_public_inputs must match extracted G1 scalar-mul constraints"
            );

            if enable_shift_g1_scalar_mul {
                let mut pairs: Vec<(VirtualPolynomial, Vec<Fq>, VirtualPolynomial, Vec<Fq>)> =
                    Vec::with_capacity(g1_scalar_mul_constraints_tuples.len() * 2);

                for w in &g1_scalar_mul_constraints_tuples {
                    let i = w.constraint_index;
                    let xa = VirtualPolynomial::g1_scalar_mul_xa(i);
                    let xa_next = VirtualPolynomial::g1_scalar_mul_xa_next(i);
                    pairs.push((xa, w.x_a.clone(), xa_next, w.x_a_next.clone()));

                    let ya = VirtualPolynomial::g1_scalar_mul_ya(i);
                    let ya_next = VirtualPolynomial::g1_scalar_mul_ya_next(i);
                    pairs.push((ya, w.y_a.clone(), ya_next, w.y_a_next.clone()));
                }

                let shift_params = g1_shift_params(pairs.len());
                let shift_prover =
                    ShiftG1ScalarMulProver::<Fq, T>::new(shift_params, pairs, transcript);
                provers.push(Box::new(shift_prover));
            }

            let mut g1_scalar_mul_constraints: Vec<G1ScalarMulConstraintPolynomials<Fq>> =
                Vec::with_capacity(g1_scalar_mul_constraints_tuples.len());
            let mut g1_scalar_mul_base_points: Vec<(Fq, Fq)> =
                Vec::with_capacity(g1_scalar_mul_constraints_tuples.len());

            for w in g1_scalar_mul_constraints_tuples {
                g1_scalar_mul_constraints.push(G1ScalarMulConstraintPolynomials {
                    x_a: w.x_a,
                    y_a: w.y_a,
                    x_t: w.x_t,
                    y_t: w.y_t,
                    x_a_next: w.x_a_next,
                    y_a_next: w.y_a_next,
                    t_indicator: w.t_indicator,
                    a_indicator: w.a_indicator,
                    constraint_index: w.constraint_index,
                });
                g1_scalar_mul_base_points.push(w.base_point);
            }

            let params = G1ScalarMulParams::new(g1_scalar_mul_constraints.len());
            let (spec, constraint_indices) = G1ScalarMulProverSpec::new(
                params,
                g1_scalar_mul_constraints,
                &self.constraint_system.g1_scalar_mul_public_inputs,
                g1_scalar_mul_base_points,
            );
            let prover = G1ScalarMulProver::from_spec(spec, constraint_indices, transcript);
            provers.push(Box::new(prover));
        }

        // G2 scalar mul
        let g2_scalar_mul_constraints_tuples =
            self.constraint_system.extract_g2_scalar_mul_constraints();
        if !g2_scalar_mul_constraints_tuples.is_empty() {
            debug_assert_eq!(
                self.constraint_system.g2_scalar_mul_public_inputs.len(),
                g2_scalar_mul_constraints_tuples.len(),
                "ConstraintSystem.g2_scalar_mul_public_inputs must match extracted G2 scalar-mul constraints"
            );

            if enable_shift_g2_scalar_mul {
                let mut pairs: Vec<(VirtualPolynomial, Vec<Fq>, VirtualPolynomial, Vec<Fq>)> =
                    Vec::with_capacity(g2_scalar_mul_constraints_tuples.len() * 4);
                for w in &g2_scalar_mul_constraints_tuples {
                    let i = w.constraint_index;
                    let xa_c0 = VirtualPolynomial::g2_scalar_mul_xa_c0(i);
                    let xa_next_c0 = VirtualPolynomial::g2_scalar_mul_xa_next_c0(i);
                    pairs.push((xa_c0, w.x_a_c0.clone(), xa_next_c0, w.x_a_next_c0.clone()));

                    let xa_c1 = VirtualPolynomial::g2_scalar_mul_xa_c1(i);
                    let xa_next_c1 = VirtualPolynomial::g2_scalar_mul_xa_next_c1(i);
                    pairs.push((xa_c1, w.x_a_c1.clone(), xa_next_c1, w.x_a_next_c1.clone()));

                    let ya_c0 = VirtualPolynomial::g2_scalar_mul_ya_c0(i);
                    let ya_next_c0 = VirtualPolynomial::g2_scalar_mul_ya_next_c0(i);
                    pairs.push((ya_c0, w.y_a_c0.clone(), ya_next_c0, w.y_a_next_c0.clone()));

                    let ya_c1 = VirtualPolynomial::g2_scalar_mul_ya_c1(i);
                    let ya_next_c1 = VirtualPolynomial::g2_scalar_mul_ya_next_c1(i);
                    pairs.push((ya_c1, w.y_a_c1.clone(), ya_next_c1, w.y_a_next_c1.clone()));
                }

                let shift_params = g2_shift_params(pairs.len());
                let shift_prover =
                    ShiftG2ScalarMulProver::<Fq, T>::new(shift_params, pairs, transcript);
                provers.push(Box::new(shift_prover));
            }

            let mut g2_scalar_mul_constraints: Vec<G2ScalarMulConstraintPolynomials<Fq>> =
                Vec::with_capacity(g2_scalar_mul_constraints_tuples.len());
            let mut g2_scalar_mul_base_points =
                Vec::with_capacity(g2_scalar_mul_constraints_tuples.len());

            for w in g2_scalar_mul_constraints_tuples {
                g2_scalar_mul_constraints.push(G2ScalarMulConstraintPolynomials {
                    x_a_c0: w.x_a_c0,
                    x_a_c1: w.x_a_c1,
                    y_a_c0: w.y_a_c0,
                    y_a_c1: w.y_a_c1,
                    x_t_c0: w.x_t_c0,
                    x_t_c1: w.x_t_c1,
                    y_t_c0: w.y_t_c0,
                    y_t_c1: w.y_t_c1,
                    x_a_next_c0: w.x_a_next_c0,
                    x_a_next_c1: w.x_a_next_c1,
                    y_a_next_c0: w.y_a_next_c0,
                    y_a_next_c1: w.y_a_next_c1,
                    t_indicator: w.t_indicator,
                    a_indicator: w.a_indicator,
                    constraint_index: w.constraint_index,
                });
                g2_scalar_mul_base_points.push(w.base_point);
            }

            let params = G2ScalarMulParams::new(g2_scalar_mul_constraints.len());
            let (spec, constraint_indices) = G2ScalarMulProverSpec::new(
                params,
                g2_scalar_mul_constraints,
                &self.constraint_system.g2_scalar_mul_public_inputs,
                g2_scalar_mul_base_points,
            );
            let prover = G2ScalarMulProver::from_spec(spec, constraint_indices, transcript);
            provers.push(Box::new(prover));
        }

        // G1 add
        let g1_add_constraints = self.constraint_system.extract_g1_add_constraints();
        if !g1_add_constraints.is_empty() {
            // NOTE: The fused G1Add sumcheck exists (see `g1::fused_addition`) but is intentionally
            // NOT wired into the default prover pipeline right now.
            let params = G1AddParams::new(g1_add_constraints.len());
            let (spec, constraint_indices) = G1AddProverSpec::new(params, g1_add_constraints);
            let prover = G1AddProver::from_spec(spec, constraint_indices, transcript);
            provers.push(Box::new(prover));
        }

        // G2 add
        let g2_add_constraints = self.constraint_system.extract_g2_add_constraints();
        if !g2_add_constraints.is_empty() {
            let params = G2AddParams::new(g2_add_constraints.len());
            let (spec, constraint_indices) = G2AddProverSpec::new(params, g2_add_constraints);
            let prover = G2AddProver::from_spec(spec, constraint_indices, transcript);
            provers.push(Box::new(prover));
        }

        // Multi-Miller loop (pairing Miller loop) + shift chaining (experimental)
        #[cfg(feature = "experimental-pairing-recursion")]
        {
            let mml_constraints = self
                .constraint_system
                .extract_multi_miller_loop_constraints();
            if !mml_constraints.is_empty() {
                if enable_shift_multi_miller_loop {
                    let mut pairs: Vec<(VirtualPolynomial, Vec<Fq>, VirtualPolynomial, Vec<Fq>)> =
                        Vec::with_capacity(mml_constraints.len() * 5);
                    for w in &mml_constraints {
                        let i = w.constraint_index;
                        pairs.push((
                            VirtualPolynomial::multi_miller_loop_f(i),
                            w.f.clone(),
                            VirtualPolynomial::multi_miller_loop_f_next(i),
                            w.f_next.clone(),
                        ));
                        pairs.push((
                            VirtualPolynomial::multi_miller_loop_t_x_c0(i),
                            w.t_x_c0.clone(),
                            VirtualPolynomial::multi_miller_loop_t_x_c0_next(i),
                            w.t_x_c0_next.clone(),
                        ));
                        pairs.push((
                            VirtualPolynomial::multi_miller_loop_t_x_c1(i),
                            w.t_x_c1.clone(),
                            VirtualPolynomial::multi_miller_loop_t_x_c1_next(i),
                            w.t_x_c1_next.clone(),
                        ));
                        pairs.push((
                            VirtualPolynomial::multi_miller_loop_t_y_c0(i),
                            w.t_y_c0.clone(),
                            VirtualPolynomial::multi_miller_loop_t_y_c0_next(i),
                            w.t_y_c0_next.clone(),
                        ));
                        pairs.push((
                            VirtualPolynomial::multi_miller_loop_t_y_c1(i),
                            w.t_y_c1.clone(),
                            VirtualPolynomial::multi_miller_loop_t_y_c1_next(i),
                            w.t_y_c1_next.clone(),
                        ));
                    }

                    let shift_params = ShiftMultiMillerLoopParams::new(pairs.len());
                    let shift_prover =
                        ShiftMultiMillerLoopProver::<Fq, T>::new(shift_params, pairs, transcript);
                    provers.push(Box::new(shift_prover));
                }

                let params = MultiMillerLoopParams::new(mml_constraints.len());
                let (spec, constraint_indices) =
                    MultiMillerLoopProverSpec::new(params, mml_constraints);
                let prover = MultiMillerLoopProver::from_spec(spec, constraint_indices, transcript);
                provers.push(Box::new(prover));
            }
        }

        // TODO: Add wiring/boundary constraints sumcheck (AST-driven).

        if provers.is_empty() {
            return Err("No constraints to prove in Stage 2".into());
        }

        let (proof, r_stage2) = BatchedSumcheck::prove(
            provers.iter_mut().map(|p| &mut **p as _).collect(),
            accumulator,
            transcript,
        );

        Ok((proof, r_stage2))
    }

    /// Run Stage 3: Prefix packing reduction.
    ///
    /// This samples fresh "packing prefix" challenges and reduces all Stage 2 virtual openings
    /// to a single opening of the packed dense polynomial `CommittedPolynomial::DoryDenseMatrix`.
    #[tracing::instrument(skip_all, name = "RecursionProver::prove_stage3_prefix_packing")]
    pub(crate) fn prove_stage3_prefix_packing<T: Transcript>(
        &mut self,
        transcript: &mut T,
        accumulator: &mut ProverOpeningAccumulator<Fq>,
        metadata: &RecursionConstraintMetadata,
        r_x: &[<Fq as JoltField>::Challenge],
    ) -> Result<Fq, Box<dyn std::error::Error>> {
        // We no longer need the sparse matrix after Stage 2 (the PCS commitment is already bound).
        let _dropped_matrix_evals = std::mem::take(&mut self.constraint_system.matrix.evaluations);

        // Derive the public packing layout.
        let layout = PrefixPackingLayout::from_constraint_types(&metadata.constraint_types);
        if layout.num_dense_vars != metadata.dense_num_vars {
            return Err(format!(
                "prefix packing layout mismatch: metadata.dense_num_vars={} but derived layout has {}",
                metadata.dense_num_vars, layout.num_dense_vars
            )
            .into());
        }

        let max_native_vars = layout.entries.iter().map(|e| e.num_vars).max().unwrap_or(0);
        if r_x.len() < max_native_vars {
            return Err(format!(
                "Stage 2 produced r_x of length {}, but prefix packing needs at least {} bits",
                r_x.len(),
                max_native_vars
            )
            .into());
        }

        // Low variables: shared `r_x` portion (suffix-aligned Stage 2), **reversed**.
        //
        // Stage 2 constraint sumchecks are suffix-aligned in the batched sumcheck, so an m-var
        // polynomial is opened at the *suffix* of the common 11-var challenge vector. For the
        // prefix-packing reduction we want these to appear as prefixes, so we reverse `r_x` here.
        let mut r_x_fq: Vec<Fq> = r_x[r_x.len() - max_native_vars..]
            .iter()
            .map(|c| (*c).into())
            .collect();
        r_x_fq.reverse();

        // High variables: fresh packing challenges.
        let pack_len = layout.num_dense_vars.saturating_sub(max_native_vars);
        let r_pack: Vec<Fq> = (0..pack_len)
            .map(|_| transcript.challenge_scalar::<Fq>())
            .collect();

        // Full packed opening point in little-endian (low-to-high) variable order.
        let mut r_full_lsb: Vec<Fq> = Vec::with_capacity(layout.num_dense_vars);
        r_full_lsb.extend_from_slice(&r_x_fq);
        r_full_lsb.extend_from_slice(&r_pack);

        // Extract Stage 2 virtual claims in the standard [constraint-major, poly-type-minor] layout.
        let virtual_claims = extract_virtual_claims_from_accumulator(
            accumulator,
            &metadata.constraint_types,
            &self.constraint_system.gt_exp_public_inputs,
        );
        let num_poly_types = PolyType::NUM_TYPES;

        // Compute packed evaluation claim: F(r) = Σ_i eq(prefix_i, codeword_i) · f_i(r_x_prefix).
        let packed_eval =
            packed_eval_from_claims(&layout, &r_full_lsb, |constraint_idx, poly_type| {
                let claim_idx = constraint_idx * num_poly_types + (poly_type as usize);
                virtual_claims
                    .get(claim_idx)
                    .copied()
                    .unwrap_or_else(Fq::zero)
            });

        // Append as a Fiat–Shamir message (matches existing Stage 3 transcript pattern).
        transcript.append_scalar(&packed_eval);

        // Register a committed opening on the packed dense polynomial.
        //
        // Opening points are stored in BIG_ENDIAN order for PCS verification.
        let opening_point: Vec<<Fq as JoltField>::Challenge> =
            r_full_lsb.into_iter().rev().map(|f| f.into()).collect();
        accumulator.append_dense(
            transcript,
            CommittedPolynomial::DoryDenseMatrix,
            SumcheckId::RecursionPacked,
            opening_point,
            packed_eval,
        );

        Ok(packed_eval)
    }
}
